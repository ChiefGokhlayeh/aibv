{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop zu Image Segmentation am Beispiel U-Net\n",
    "\n",
    "Autor: Joerg Dahlkemper\n",
    "Version: 2022-06-15\n",
    "\n",
    "Idee: https://pyimagesearch.com/2022/02/21/u-net-image-segmentation-in-keras/ und https://keras.io/examples/vision/oxford_pets_image_segmentation/\n",
    "\n",
    "Literatur:\n",
    "- U-Net Basisartikel https://arxiv.org/abs/1505.04597 \n",
    "- DeepLab V3 https://arxiv.org/abs/1706.05587 \n",
    "- HRNet https://arxiv.org/abs/1908.07919 \n",
    "- U2-Net: https://arxiv.org/abs/2005.09007 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitung\n",
    "\n",
    "Neben Tensorflow werden insbesondere die Bibliotheken labelme und albumentations benötigt, diese sind per pip einzubinden:\n",
    "\n",
    "```pip install albumentations```<br>\n",
    "```pip install labelme```\n",
    "\n",
    "- Testen Sie anschließend die Ausführung der nachfolgenden imports und installieren Sie ggfs. fehlende Bibliotheken per ```pip install``` nach.\n",
    "- Gehen Sie in Ihr lokals Verzeichnis des git-Repos ```aibv``` und führen Sie ```git pull``` aus, um die aktuellen Workshopdateien zu laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import labelme\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten aufbereiten: Labeln\n",
    "\n",
    "- üblicherweise werden Bilddaten manuell per ```labelme``` annotiert, siehe https://github.com/wkentaro/labelme und dortige Verweise auf Tutorials (siehe dazu auch https://olafenwaayoola.medium.com/image-annotation-with-labelme-81687ac2d077 )\n",
    "- in den damit erzeugten JSON-Dateien werden sowohl die Originalbilder als auch die Masken gespeichert\n",
    "\n",
    "Labelme ist eine Python-Bibliothek, die in ein Python-Script eingebunden werden kann und Hilfsfunktionen, wie die Extraktion der Bildinformationen aus der JSON-Datei bietet. Das Programm zur Annotation von Bilddaten kann aber auch direkt aufgerufen werden, indem in der Eingabeaufforderung auf das gewünschte Bildverzeichnis gewechselt wird und dort Labelme als Python-Modul ausgeführt wird:\n",
    "\n",
    "```python -m labelme```\n",
    "\n",
    "**Aufgabe 1: Labeln eines Bildes**<br>\n",
    "Aktualisieren Sie das Git-Repo der Vorlesung, indem Sie in das Verzeichnis ```aibv``` des git-Repos auf Ihrem Rechner wechseln und ```git pull``` ausführen. Labeln Sie eines der Bilder in dem Ordner ```aibv/workshop/tobelabeld``` entsprechend Ihrer Platznummer. Verwenden Sie dazu je nach Solarzellen einen oder mehrere der Label *crack*, *darkarea*, *finger*. Speichern Sie das Ergebnis und analysieren Sie die resultierende JSON-Datei mit VS Code oder einem anderen Texteditor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten aufbereiten: Masken generieren\n",
    "\n",
    "Zur Extraktion des Originalbildes und der Masken aus der JSON-Datei kann entweder auf fertige Skripte im Internet zurückgegriffen werden. Das Grundprinzip zur Erzeugung einer Maske kann aber auch sehr einfach durch Extraktion der in der JSON-Datei gespeicherten Punkte und Zeichnen eines entsprechenden Polygonzugs erfolgen, wie nachfolgend mit OpenCV und der Fuktion polyfill dargestellt wird.\n",
    "\n",
    "**Aufgabe 2: Maske erzeugen**<br>\n",
    "Analysieren Sie die Funktionsweise des folgenden Skriptbeispiels und testen Sie dies an dem von Ihnen gelabelten Bild. Erstellen Sie dazu ein Ablaufdiagramm für das Zeichnen der Labels und vergleichen Sie dies mit der Datenstruktur Ihres gelabelten JSON-Files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILE = \"tobelabeled/0009.json\"\n",
    "\n",
    "\n",
    "def label_scan(filename):\n",
    "\n",
    "    labels = set()  # data structure with unique elements to avoid duplicates\n",
    "\n",
    "    # read image and create a labelmap with background\n",
    "    label_file = labelme.LabelFile(filename=JSON_FILE)\n",
    "    img = labelme.utils.img_data_to_arr(label_file.imageData)  # noqa: F841\n",
    "\n",
    "    with open(JSON_FILE) as json_file:\n",
    "        json_label = json.loads(json_file.read())\n",
    "\n",
    "        # walk trough all classes and generate label list\n",
    "        for classes in json_label[\"shapes\"]:\n",
    "            labels.add(classes[\"label\"])\n",
    "\n",
    "    label_list = sorted(labels)\n",
    "    print(\"[i] Labels found:\", label_list)\n",
    "    return label_list\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    labels = label_scan(JSON_FILE)\n",
    "\n",
    "    # read image and create a labelmap with background\n",
    "    label_file = labelme.LabelFile(filename=JSON_FILE)\n",
    "    img = labelme.utils.img_data_to_arr(label_file.imageData)\n",
    "    img_file_name = JSON_FILE.split(\".\")[0] + \"_from_json.jpg\"\n",
    "    mask_file_name = JSON_FILE.split(\".\")[0] + \".png\"\n",
    "\n",
    "    with open(JSON_FILE) as json_file:\n",
    "        json_label = json.loads(json_file.read())\n",
    "        img_height, img_width = img.shape\n",
    "        label_map = np.zeros((img_height, img_width), np.uint8)\n",
    "\n",
    "        for classes in json_label[\"shapes\"]:\n",
    "\n",
    "            # fill contour\n",
    "\n",
    "            for i, label in enumerate(labels):\n",
    "                if classes[\"label\"] == label:\n",
    "                    points = np.asarray(classes[\"points\"], dtype=int)\n",
    "                    cv2.fillPoly(label_map, pts=[points], color=i + 1)\n",
    "                    print(\"[i] added segment\", label)\n",
    "\n",
    "        cv2.imwrite(img_file_name, img)\n",
    "        cv2.imwrite(mask_file_name, label_map)\n",
    "\n",
    "        plt.suptitle(JSON_FILE)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"Image\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(label_map)\n",
    "        plt.title(\"Mask\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einlesen\n",
    "\n",
    "Es wird nachfolgend auf bereits gelabelte Bilder zugegriffen, die im git-Repo im Ordner segmentation abgelegt sind. In diesem Ordner befindet sich ein Ordner *image* für die Originalbilder und ein Ordner *masks* für die zugehörigen Masken.\n",
    "\n",
    "**Aufgabe 3: Daten einlesen**<br>\n",
    "Analysieren Sie die Codezeile zur Erstellung einer Liste der Bildpfade und lesen Sie in gleicher Weise die Liste der Masken ein. Machen Sie sich klar, dass die Sortierung später unbedingt erforderlich ist, da über den Listenindex auf das Bild und die zugehörige Maske zugegriffen wird.\n",
    "\n",
    "Die Bild | Masken - Paare werden anschließend ausgegeben. Machen Sie sich klar, was die Funktion ```zip()``` in diesem typischen Anwendungsfall bewirkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"segmentation/images\"\n",
    "MASK_DIR = \"segmentation/masks\"\n",
    "\n",
    "image_paths = sorted(\n",
    "    os.path.join(IMAGE_DIR, file_name)\n",
    "    for file_name in os.listdir(IMAGE_DIR)\n",
    "    if file_name.endswith(\".jpg\")\n",
    ")\n",
    "\n",
    "# TODO: Komplettieren Sie den fehlenden Code zur Erzeugung der Liste der Masken-Dateien\n",
    "mask_paths = \"TODO\"\n",
    "\n",
    "print(\"[i] Number of samples:\", len(image_paths))\n",
    "\n",
    "for image_path, mask_path in zip(image_paths, mask_paths):\n",
    "    print(image_path, \"|\", mask_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe 4: Bildpaar anzeigen**<br>\n",
    "\n",
    "- Lesen Sie dazu das Bild und die Maske über die Keras-Fumktion ```load_img``` ein und nutzen Sie bei der Maske den Modus ```load_img(..., color_mode='grayscale')```.\n",
    "- Hinweis: Wenn Sie das Bild mit ```plt.imread``` einlesen, wird der ursprüngliche Wertebereich 0 ... 2 der Labels auf ein 255-stel verändert.\n",
    "- Analysieren Sie die Form der Bilddaten und der Maske (wieviel Kanäle hat das Bild, welche Wertebereiche)\n",
    "- Zeigen Sie unter Nutzung von matplotlib eines der Paare von Bild und Maske an. \n",
    "- Prüfen Sie, ob die Maske und das Bild zusammenpassen.\n",
    "- Nutzen Sie bei der Anzeige der Maske, die Möglichkeit, den Farbbereich auf die Werte der Labels per ```imshow(..., vmin=0, vmax=2)``` einzugrenzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Anzeige eines bestimmten Bildpaares, z.B. 42 und überprüfen, ob Maske und Bild zusammenpassen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bild-Datengenerator ImageGenerator\n",
    "\n",
    "Die Daten sind in keras-geeignete Datenstruktur zu überführen und Data augmentation ist erforderlich. Der imageDataGenerator von Keras erlaubt aber keine simultane Data Augmentation auf Bild und Maske. Für eine solche simultane Data Augmentation wird häufig die Bibliothek *albumentations* eingesetzt. Nachfolgend wird ein eigener Bilddaten-Generator implementiert.\n",
    "\n",
    "**Aufgabe 5: Albumentations**<br>\n",
    "Analysiseren Sie, wie das gleichzeitige Ändern von Bild und Maske implementiert wird, indem Sie die Dokumentationsseite von Albumentations aufrufen:\n",
    "https://albumentations.ai/docs/getting_started/mask_augmentation/ \n",
    "Analysieren Sie die Funktion des nachfolgenden ImageGenerators und machen Sie sich klar, welche Art von Data Augmentation ausgeführt wird und ergänzen Sie die simultane Änderung von Bilddaten und zugehöriger Maske."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Helper allowing to iterate over the image data via for loop or next()\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, image_paths, mask_paths, augment=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.augment = augment  # flag for activation of the following augmentations\n",
    "        self.transform = A.Compose(\n",
    "            [\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.2),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"returns the number of steps to cover the full data set\"\"\"\n",
    "        return len(self.image_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"returns tuple (input, mask) corresponing to batch index\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_image_paths = self.image_paths[i : i + self.batch_size]\n",
    "        batch_mask_paths = self.mask_paths[i : i + self.batch_size]\n",
    "\n",
    "        # create empty np-arrays for image (x) and label mask (y)\n",
    "        x = np.zeros(\n",
    "            (self.batch_size,) + self.img_size + (3,), dtype=\"float32\"\n",
    "        )  # concatenation of tupels to fit shape requirements\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "\n",
    "        for j, (img_path, mask_path) in enumerate(\n",
    "            zip(batch_image_paths, batch_mask_paths)\n",
    "        ):\n",
    "            img = (\n",
    "                np.array(load_img(img_path, target_size=self.img_size), dtype=\"float32\")\n",
    "                / 255\n",
    "            )\n",
    "            mask = np.array(\n",
    "                load_img(mask_path, target_size=self.img_size, color_mode=\"grayscale\"),\n",
    "                dtype=\"uint8\",\n",
    "            )\n",
    "            mask = np.expand_dims(\n",
    "                mask, 2\n",
    "            )  # additional dimension for class number to fit required shape\n",
    "\n",
    "            if self.augment:\n",
    "                # TODO: Implementieren Sie hier die simultane Data augmentation gemäß der Anleitung von Albumentations\n",
    "\n",
    "                img = \"TODO\"  # noqa: F841\n",
    "                mask = \"TODO\"\n",
    "\n",
    "            x[j] = img\n",
    "            y[j] = mask  # additional dimension for class number to fit required shape\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zugriff auf ImageGenerator\n",
    "\n",
    "Der ImageGenerator erbt von der Klasse ```Sequence``` und implementiert die Methode ```__getitem__```, so dass die Batches per for-Schleife abgerufen werden können.\n",
    "\n",
    "**Aufgabe 6: Anzeige eines Batches**<br>\n",
    "Erstellen Sie einen ImageGenerator für eine Batch size von 3 und zeigen Sie die drei Bildpaare unter Nutzung von ```plt.subplot``` übersichtlich an. Dazu bietet es sich an, die For-Schleife nach einem Durchlauf per ```break``` zu beenden. Bei der Anzeige der Maske ist es notwendig, den Farbbereich auf dem Wertebereich der Labels, also 0 ... 2 per Option ```imshow(... ,vmin=0, vmax=2)``` einzuschränken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ImageGenerator erzeugen und drei Bildpaare anzeigen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufteilung von Trainings- und Validierungsdaten und Testdaten\n",
    "\n",
    "Eine übliche Aufteilung wäre 70 % Trainingsbilder, 15% Validierungsbilder, 15% Testbilder (nur einmalig ganz am Projektende zu verwenden). Da hier nur eine sehr geringe Anzahl von 72 = 6 x 12 Bildern vorliegt, wird genau ein batch als Validierungsdatensatz genutzt, der Rest als Trainingsdaten. Es werden keine Testdaten vorgehalten.\n",
    "\n",
    "\n",
    "**Aufgabe 7: Codeanalyse Data splitting**<br>\n",
    "- Erklären Sie, wieso die Konstante SEED erforderlich ist.\n",
    "- Visualisieren Sie als Skizze, wie die Aufteilung von Trainings- und Validierungsdaten per Doppelpunkt-Operator gelöst ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12  # depends on available memory\n",
    "HEIGHT = 128\n",
    "WIDTH = 128\n",
    "SEED = 42\n",
    "\n",
    "val_samples = 12\n",
    "\n",
    "# Bilder und Masken zufällig anordnen\n",
    "random.Random(SEED).shuffle(image_paths)\n",
    "random.Random(SEED).shuffle(mask_paths)\n",
    "\n",
    "train_image_paths = image_paths[:-val_samples]\n",
    "train_mask_paths = mask_paths[:-val_samples]\n",
    "val_image_paths = image_paths[-val_samples:]\n",
    "val_mask_paths = mask_paths[-val_samples:]\n",
    "\n",
    "# instantiate training and validation generators\n",
    "train_gen = ImageGenerator(\n",
    "    BATCH_SIZE, (HEIGHT, WIDTH), train_image_paths, train_mask_paths, True\n",
    ")\n",
    "val_gen = ImageGenerator(\n",
    "    BATCH_SIZE, (HEIGHT, WIDTH), val_image_paths, val_mask_paths, False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsprogramm zur Visualisierung der Daten\n",
    "\n",
    "Je nachdem, ob nur ein Bildpaar (Original, Maske) oder ein Bildtriple (Original, Maske, Prediction) angezeigt werden soll, soll nachfolgende Funktion eine Liste von Originalbild, Maske und ggfs. dem Ergebnis der Segmentierung zur Anzeige bringen. Die Liste kann 2 oder 3 Bilder enthalten.\n",
    "\n",
    "**Aufgabe 8: Analyse des Imshow-Befehls**<br>\n",
    "Analysieren Sie, wozu die Methode ```squeeze()``` dient. Testen Sie dazu den nachfolgenden Code auch ohne diesen Methodenaufruf und interpretieren Sie die Fehlermeldung von ```imshow```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    title = [\"Input Image\", \"True Mask\", \"Predicted Mask\"]\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i + 1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow((display_list[i]).squeeze(), vmin=0, vmax=2)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "sample_batch = next(\n",
    "    iter(train_gen)\n",
    ")  # train_batches does not support next, thus to be converted into iterable\n",
    "random_index = np.random.choice(sample_batch[0].shape[0])\n",
    "sample_image, sample_mask = sample_batch[0][random_index], sample_batch[1][random_index]\n",
    "display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell erstellen\n",
    "\n",
    "Das Modell besteht aus dem U-förmigen Encoder und Decoder (auch Bottleneck genannt). Hierzu werden Conv-Layer mit der Aktivierungsfunktion ReLU genutzt. Es werden stets zwei Conv-Layer gleicher Filtergröße hintereinandergeschaltet, dazu bietet sich die Erstellung einer Funktion an. Darüber hinaus werden Blöcke für den Encoder zum Downsampling und Blöcke für den Decoder mit Upsampling benötigt, für die ebenfalls eigenständige Funktionen erstellt werden.\n",
    "\n",
    "Da zur Erstellung der Skip Connections eine Concatenation erforderlich ist, bietet sich die functional API von Keras an.\n",
    "\n",
    "**Aufgabe 9: U-Net Architektur ergänzen**<br>\n",
    "Machen Sie sich anhand der Vorlesungsunterlagen den Aufbau des U-Nets klar und vervollständigen Sie den Encoder-Teil des nachfolgenden Codes um die fehlenden Layer. Die korrekte Umsetzung können Sie durch die übernächste Code-Zeile prüfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv_block(x, n_filters):\n",
    "    \"\"\"generate sequence of 2 conv layers with same filter size\"\"\"\n",
    "    x = layers.Conv2D(\n",
    "        n_filters, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"\n",
    "    )(x)\n",
    "    x = layers.Conv2D(\n",
    "        n_filters, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"\n",
    "    )(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def downsample_block(x, n_filters):\n",
    "    \"\"\"generate downsample block for encoder\"\"\"\n",
    "    f = double_conv_block(x, n_filters)\n",
    "    p = layers.MaxPool2D(2)(f)\n",
    "    p = layers.Dropout(0.3)(p)\n",
    "    return f, p\n",
    "\n",
    "\n",
    "def upsample_block(x, conv_features, n_filters):\n",
    "    \"\"\"generate upsample block for decoder\"\"\"\n",
    "    x = layers.Conv2DTranspose(n_filters, kernel_size=3, strides=2, padding=\"same\")(\n",
    "        x\n",
    "    )  # upsampling\n",
    "    x = layers.concatenate([x, conv_features])  # concatenation for skip connection\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = double_conv_block(x, n_filters)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_unet_model(num_classes):\n",
    "    \"\"\"generation of unet model with decoder, bottleneck and encoder\"\"\"\n",
    "    inputs = layers.Input(shape=(128, 128, 3))\n",
    "\n",
    "    # encoder with downsampling\n",
    "    f1, p1 = downsample_block(inputs, 64)\n",
    "    f2, p2 = downsample_block(p1, 128)\n",
    "    f3, p3 = downsample_block(p2, 256)\n",
    "    f4, p4 = downsample_block(p3, 512)\n",
    "\n",
    "    # bottleneck\n",
    "    bottleneck = double_conv_block(p4, 1024)\n",
    "\n",
    "    # decoder with upsampling\n",
    "    u6 = upsample_block(bottleneck, f4, 512)  # noqa: F841\n",
    "    # TODO: vervollstaendigen Sie die fehlenden Layer des U-Nets\n",
    "    u7 = \"TODO\"  # noqa: F841\n",
    "    u8 = \"TODO\"  # noqa: F841\n",
    "    u9 = \"TODO\"  # noqa: F841\n",
    "\n",
    "    # outputs\n",
    "    outputs = layers.Conv2D(num_classes, 3, padding=\"same\", activation=\"softmax\")(u9)\n",
    "\n",
    "    # functional definition of model\n",
    "    unet_model = keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "\n",
    "    return unet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei erfolgreicher Umsetzung lässt sich das Modell hiermit bauen und es ergibt bei dem Aufruf von ```unet_model.summary()``` sich die Anzahl von 34.515.011 zu trainierenden Parametern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = build_unet_model(3)\n",
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Für die semantische Segmentierung bei sich ausschließenden Klassen (jedes Pixel kann nur einer Klasse zugeordnet werden), wird in der Regel die Kostenfunktion ```sparse_categorical_crossentropy``` verwendet.\n",
    "\n",
    "Je nach Leistungsfähigkeit des Rechners variiert die Ausführungszeit je Epoche von etwa 70 ms bei Einsatz einer Graphikkarte bis zu 10 s. Entsprechend ist nachfolgendes Training zunächst mit einer geringen Anzahl von Epochen zu starten. Erste brauchbare Ergebnisse sind bereits bei 50 Epochen zu erwarten. Wünschenswert ist ein Training über 200 Epochen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5  # ab 50 Epochen brauchbare Werte, 200 Epochen erstrebenswert, entsprechend anpassen\n",
    "\n",
    "unet_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=\"accuracy\",\n",
    ")\n",
    "model_history = unet_model.fit(train_gen, epochs=NUM_EPOCHS, validation_data=val_gen)\n",
    "unet_model.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung des Trainingsverlaufs\n",
    "\n",
    "Nachfolgend wird der Trainingsverlauf dargestellt.\n",
    "\n",
    "**Aufgabe 10: Trainingsverlauf analysieren**<br>\n",
    "Analysieren Sie den unterschiedlichen Verlauf von Loss und Accuracy und überlegen Sie, was an dem Verlauf der Accuracy ungewöhnlich ist und welche Ursache dies haben kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model_history.history[\"loss\"]\n",
    "val_loss = model_history.history[\"val_loss\"]\n",
    "acc = model_history.history[\"accuracy\"]\n",
    "val_acc = model_history.history[\"val_accuracy\"]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss, label=\"loss\")\n",
    "plt.plot(val_loss, label=\"val_loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(acc, label=\"acc\")\n",
    "plt.plot(val_acc, label=\"val_acc\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Sofern das Modell bereits erstellt und abgespeichert wurde kann es durch Ausführen der folgenden Zelle eingelesen werden, ohne das Training zu wiederholen. Wenn das eigene Training zu langsam ist, können Sie auf das im Git-Repo hinterlegte Modell ``` master_model.h5``` zugreifen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet_model = keras.models.load_model(\"my_model.h5\")\n",
    "# unet_model = keras.models.load_model(\"master_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung der Ergebnisse\n",
    "\n",
    "Um einen qualitativen Eindruck der Leistungsfähigkeit der Lösung zu erhalten, sollen die Ergebnisse der Vorhersage auf dem Validierungsdatensatz visualisiert werden. Hierzu wird nachfolgend eine Prediction auf dem Validierungsdatensatz ausgeführt.\n",
    "\n",
    "**Aufgabe 11: Analyse des Ausgabeformats**<br>\n",
    "Werten Sie jeweils von der Maske und der vorhergesagten Maske die Anzahl der Dimensionen und deren Wertebereiche aus und erklären Sie den Unterschied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, masks = next(\n",
    "    iter(val_gen)\n",
    ")  # convert val_gen in iterable to get single batch by next\n",
    "predicted_masks = unet_model.predict(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analysieren Sie die shape der Tensoren images, masks und predicted_masks und den Wertebereich von predicted_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe 12: Umwandlung der Vorhersage**<br>\n",
    "Analysieren Sie, wie die Umwandlung der vorhergesagten Maske in der Funktion ```multi2one_channel_mask``` funktioniert und führen Sie dann die nachfolgende Zelle aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi2one_channel_mask(multi_channel_mask):\n",
    "    \"\"\"\n",
    "    image returns one channel per class with probability values, to display\n",
    "    the object and outline, just the channel with the highest value is selected\n",
    "    \"\"\"\n",
    "    one_channel_mask = np.argmax(multi_channel_mask, axis=-1)\n",
    "    output_mask = np.expand_dims(one_channel_mask, axis=-1)\n",
    "    return output_mask\n",
    "\n",
    "\n",
    "for i in range(len(images)):\n",
    "    display(\n",
    "        [images[i], masks[i], multi2one_channel_mask(predicted_masks[i])]\n",
    "    )  # 3 channel output converted to 1 channel with 3 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswertung\n",
    "\n",
    "**Aufgabe 13: Bewertung der Ergebnisse**<br>\n",
    "\n",
    "1. Bewerten Sie die Ergebnisse der semantischen Segmentierung qualitativ und vergleichen Sie diese vor dem Hintergrund der Segmentierung mittels klassischer Bildverarbeitung.\n",
    "2. Machen Sie sich klar, welche Operationen der klassischen Bildverarbeitung im Anschluss an die Segmentierung erforderlich sind, um eine finale Auswahl von fehlerhaften Solarzellen und deren Klassifizierung zu ermöglichen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
